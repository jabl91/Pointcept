{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import timeit\n",
    "\n",
    "import cupy as cp\n",
    "cp.random.seed(12)\n",
    "\n",
    "#### Portions of this were borrowed and adapted from the\n",
    "#### cuDF cheatsheet, existing cuDF documentation,\n",
    "#### and 10 Minutes to Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import cupy as cp\n",
    "import cudf\n",
    "from cuml.neighbors import NearestNeighbors\n",
    "from cuml.common.device_selection import using_device_type, set_global_device_type, get_global_device_type\n",
    "\n",
    "# import dask_cudf\n",
    "# from dask.distributed import Client, wait\n",
    "# from dask_cuda import LocalCUDACluster\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster = LocalCUDACluster(n_workers=1, threads_per_worker=512)\n",
    "# client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a \"coordinate\" dataframe\n",
    "n_points = 400000\n",
    "#df = cudf.DataFrame()\n",
    "\n",
    "point_cloud = torch.cuda.FloatTensor(n_points, 3).uniform_()\n",
    "\n",
    "# df['x'] = point_cloud[:, 0]\n",
    "# df['y'] = point_cloud[:, 1]\n",
    "# df['z'] = point_cloud[:, 2]\n",
    "\n",
    "# Generate random data for the DataFrame\n",
    "random_data = np.array(point_cloud.cpu())\n",
    "\n",
    "# Create the cudf.DataFrame\n",
    "df = cudf.DataFrame(random_data, columns=[\"X\", \"Y\", \"Z\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddf = dask_cudf.from_cudf(df, npartitions=1)\n",
    "# ddf.head()\n",
    "\n",
    "# ddf = ddf.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.datasets import make_blobs\n",
    "from time import perf_counter\n",
    "\n",
    "# # Generate random data for the DataFrame\n",
    "# random_data2 = np.random.rand(n_points, 3)\n",
    "\n",
    "# # Create the cudf.DataFrame\n",
    "# fit_df = cudf.DataFrame(random_data2, columns=[\"X\", \"Y\", \"Z\"])\n",
    "\n",
    "# fit model\n",
    "model = NearestNeighbors(n_neighbors=n_neighbors)\n",
    "model.fit(df)\n",
    "\n",
    "# start_time = perf_counter()\n",
    "\n",
    "# # get 3 nearest neighbors\n",
    "# distances_fit, indices_fit = model.kneighbors(fit_df)\n",
    "\n",
    "# end_time = perf_counter()\n",
    "# print(f\"Average execution time with blobs: {end_time-start_time:.6f} seconds\")\n",
    "\n",
    "\n",
    "start_time = perf_counter()\n",
    "\n",
    "# get 3 nearest neighbors\n",
    "distances1, indices1 = model.kneighbors(df)\n",
    "\n",
    "end_time = perf_counter()\n",
    "print(f\"Average execution time with common data: {end_time-start_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit_df.shape == df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def autoSearchAlgorithm(df_input):\n",
    "    # Create a cuML NearestNeighbors model\n",
    "    nn = NearestNeighbors(n_neighbors=n_neighbors)\n",
    "\n",
    "    # Compute the Dask DataFrame\n",
    "    # ddf_computed = ddf.compute()\n",
    "\n",
    "    # Fit the model with the input data\n",
    "    nn.fit(df_input)\n",
    "\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    # Get the nearest neighbors\n",
    "    distances2, indices2 = nn.kneighbors(df_input)\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"auto Execution time: {execution_time} seconds\")\n",
    "    # print(indices.compute())\n",
    "\n",
    "    return distances2, indices2, execution_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances2, indices2, execution_time_auto = autoSearchAlgorithm(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices1.head() == indices2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbcSearchAlgorithm(df_input):\n",
    "    # Create a cuML NearestNeighbors model\n",
    "    nn2 = NearestNeighbors(n_neighbors=n_neighbors, algorithm='rbc')\n",
    "\n",
    "    # Fit the model with the input data\n",
    "    nn2.fit(df_input)\n",
    "\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    # Get the nearest neighbors\n",
    "    distances3, indices3 = nn2.kneighbors(df_input)\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"rbc Execution time: {execution_time} seconds\")\n",
    "\n",
    "    return distances3, indices3, execution_time\n",
    "\n",
    "distances3, indices3, execution_time_rbc = rbcSearchAlgorithm(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices2.head() == indices3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "def ivfflatSearchAlgorithm(df_input, nlist_input, nprobe_input):\n",
    "    # Create a cuML NearestNeighbors model\n",
    "    nn3 = NearestNeighbors(n_neighbors=n_neighbors, algorithm='ivfflat', algo_params={'nlist': nlist_input, 'nprobe': nprobe_input})\n",
    "\n",
    "    # nn3 = NearestNeighbors(n_neighbors=n_neighbors, algorithm='ivfflat')\n",
    "\n",
    "    # Fit the model with the input data\n",
    "    nn3.fit(df_input)\n",
    "\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    # Get the nearest neighbors\n",
    "    distances4, indices4 = nn3.kneighbors(df_input)\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"ivfflat Execution time: {execution_time} seconds\")\n",
    "\n",
    "    return distances4, indices4, execution_time\n",
    "\n",
    "distances_ivfflat, indices_ivfflat, execution_time_ivfflat = ivfflatSearchAlgorithm(df, 100, 2)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # List of n_points values\n",
    "# n_points_list = [50000, 100000, 150000, 200000, 250000, 300000, 350000, 400000, 450000, 500000, 550000] # Do not use a number above 5500000 if your GPU Memory is 16GB\n",
    "\n",
    "# # List of nlist values\n",
    "# nlist_values = [50, 100, 200, 500]\n",
    "\n",
    "# # List of nprobe values\n",
    "# nprobe_values = [2, 4, 8]\n",
    "\n",
    "# # Lists to store execution times for each function\n",
    "# execution_times_ivfflat = []\n",
    "\n",
    "# # Initialize an empty list to store the rows\n",
    "# rows = []\n",
    "\n",
    "# # Loop over n_points values\n",
    "# for n_points in n_points_list:\n",
    "\n",
    "#     print(f\"Current n_points: {n_points}\")\n",
    "\n",
    "#     # Generate random data for the DataFrame\n",
    "#     point_cloud_ivfflat = torch.cuda.FloatTensor(n_points, 3).uniform_()\n",
    "\n",
    "#     # Generate random data for the DataFrame\n",
    "#     random_data_ivfflat = np.array(point_cloud_ivfflat.cpu())\n",
    "\n",
    "#     # Create the cudf.DataFrame\n",
    "#     df_ivfflat = cudf.DataFrame(random_data_ivfflat, columns=[\"X\", \"Y\", \"Z\"])\n",
    "\n",
    "#     # Loop over nlist values\n",
    "#     for nlist in nlist_values:\n",
    "\n",
    "#         # Loop over nprobe values\n",
    "#         for nprobe in nprobe_values:\n",
    "\n",
    "#             print(f\"Current nlist: {nlist}, nprobe: {nprobe}\")\n",
    "\n",
    "#             # Call the function and measure execution time\n",
    "#             distances_ivfflat, indices_ivfflat, execution_time_ivfflat = ivfflatSearchAlgorithm(df_ivfflat, nlist, nprobe)\n",
    "\n",
    "#             # Append a new row to the list\n",
    "#             rows.append({\n",
    "#                 'n_points': n_points,\n",
    "#                 'nlist': nlist,\n",
    "#                 'nprobe': nprobe,\n",
    "#                 'execution_time_ivfflat': execution_time_ivfflat\n",
    "#             })\n",
    "\n",
    "# # Create a DataFrame from the list of rows\n",
    "# execution_times_df = pd.DataFrame(rows)\n",
    "\n",
    "# # Plotting the execution times\n",
    "# fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# # Group the data by nlist and nprobe\n",
    "# grouped_data = execution_times_df.groupby(['nlist', 'nprobe'])\n",
    "\n",
    "# # Iterate over the groups and plot the execution times\n",
    "# for group, data in grouped_data:\n",
    "#     nlist, nprobe = group\n",
    "#     ax.plot(data['n_points'], data['execution_time_ivfflat'], label=f\"nlist={nlist}, nprobe={nprobe}\")\n",
    "\n",
    "# ax.set_xlabel('n_points')\n",
    "# ax.set_ylabel('Execution Time (seconds)')\n",
    "# ax.set_title('Execution Time of ivfflatSearchAlgorithm')\n",
    "# ax.legend()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# # Find the row with the lowest execution time\n",
    "# best_row = execution_times_df['execution_time_ivfflat'].idxmin()\n",
    "\n",
    "# # Extract the corresponding nlist and nprobe values\n",
    "# best_nlist = execution_times_df.loc[best_row, 'nlist']\n",
    "# best_nprobe = execution_times_df.loc[best_row, 'nprobe']\n",
    "\n",
    "# print(f\"The best nlist and nprobe combination is nlist={best_nlist}, nprobe={best_nprobe}.\")\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# # Create a DataFrame with the best_nprobe and best_nlist values\n",
    "# data = {'best_nprobe': [best_nprobe], 'best_nlist': [best_nlist]}\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# # Save the DataFrame to a CSV file\n",
    "# df.to_csv('best_values.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "df = pd.read_csv('best_values.csv')\n",
    "\n",
    "# Update the values of best_nprobe and best_nlist\n",
    "best_nprobe = df['best_nprobe'].values[0]\n",
    "best_nlist = df['best_nlist'].values[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pointops\n",
    "\n",
    "def knn_query_pointopsv1(n_neighbors_input, point_cloud_input, n_points_input):\n",
    "    n_points_tensor = torch.cuda.FloatTensor([n_points_input])\n",
    "\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    reference_index, distanceindex = pointops.knn_query(n_neighbors_input, point_cloud_input, n_points_tensor)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    reference_index_cpu = reference_index.cpu()\n",
    "\n",
    "    # print(reference_index)\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"C++ PointopsV1 KNN Execution time: {execution_time} seconds\")\n",
    "    # print(reference_index.shape)\n",
    "\n",
    "    return reference_index_cpu, execution_time, distanceindex\n",
    "\n",
    "reference_index_cpu, execution_time_pointopsv1, distanceindex_pointopsv1 = knn_query_pointopsv1(n_neighbors, point_cloud, n_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Generate a \"coordinate\" dataframe\n",
    "n_points = 400000\n",
    "#df = cudf.DataFrame()\n",
    "\n",
    "point_cloud = torch.cuda.FloatTensor(n_points, 3).uniform_()\n",
    "\n",
    "# df['x'] = point_cloud[:, 0]\n",
    "# df['y'] = point_cloud[:, 1]\n",
    "# df['z'] = point_cloud[:, 2]\n",
    "\n",
    "# Generate random data for the DataFrame\n",
    "random_data = np.array(point_cloud.cpu())\n",
    "\n",
    "# Create the cudf.DataFrame\n",
    "df = cudf.DataFrame(random_data, columns=[\"X\", \"Y\", \"Z\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "reference_index_cpu, execution_time_pointopsv1, distanceindex_pointopsv1 = knn_query_pointopsv1(n_neighbors, point_cloud, n_points)\n",
    "\n",
    "distances_ivfflat, indices_ivfflat, execution_time_ivfflat = ivfflatSearchAlgorithm(df, best_nlist, best_nprobe)\n",
    "\n",
    "# Apply square root to all elements on distanceindex_pointopsv1 to make it compatible with distances_ivfflat\n",
    "distanceindex_pointopsv1= torch.sqrt(distanceindex_pointopsv1)\n",
    "\n",
    "# Convert distanceindex_pointopsv1 to a PyTorch Tensor\n",
    "distanceindex_pointopsv1_tensor = torch.cuda.FloatTensor(distanceindex_pointopsv1)\n",
    "\n",
    "# Convert distances_ivfflat DataFrame to a Tensor\n",
    "distances_ivfflat_tensor = torch.cuda.FloatTensor(distances_ivfflat.values)\n",
    "\n",
    "# Compare the content element-wise\n",
    "are_equal = torch.allclose(distanceindex_pointopsv1_tensor, distances_ivfflat_tensor)\n",
    "\n",
    "if are_equal:\n",
    "    print(\"The content of distanceindex_pointopsv1 and distances_ivfflat is equal.\")\n",
    "else:\n",
    "    print(\"The content differs between distanceindex_pointopsv1 and distances_ivfflat.\")\n",
    "\n",
    "\n",
    "# Compare element-wise equality\n",
    "elementwise_equal = torch.eq(distanceindex_pointopsv1_tensor, distances_ivfflat_tensor)\n",
    "\n",
    "# Count the number of differences\n",
    "num_differences = torch.sum(~elementwise_equal).item()\n",
    "\n",
    "# Get the total number of items\n",
    "total_items = distanceindex_pointopsv1_tensor.numel()\n",
    "\n",
    "print(f\"Number of differences: {num_differences}\")\n",
    "print(f\"Total number of items: {total_items}\")\n",
    "\n",
    "\n",
    "# Calculate the element-wise difference between the tensors\n",
    "difference = torch.abs(distanceindex_pointopsv1_tensor - distances_ivfflat_tensor)\n",
    "\n",
    "# Calculate the max, min, and mean difference\n",
    "max_difference = difference.max().item()\n",
    "min_difference = difference.min().item()\n",
    "mean_difference = difference.mean().item()\n",
    "\n",
    "max_difference, min_difference, mean_difference\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Flatten the difference tensor and move it to CPU\n",
    "difference_flat = difference.cpu().flatten()\n",
    "\n",
    "# Filter the difference values above 0.01\n",
    "filtered_difference = difference_flat[difference_flat > 0.01]\n",
    "\n",
    "# Plot the histogram\n",
    "plt.hist(filtered_difference, bins=10, orientation='horizontal')\n",
    "\n",
    "# Set the labels and title\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Difference')\n",
    "plt.title('Distribution of Difference Tensor (Above 0.01)')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Flatten the difference tensor and move it to CPU\n",
    "difference_flat = difference.cpu().flatten()\n",
    "\n",
    "# Filter the difference values above 0.01\n",
    "filtered_difference = difference_flat[difference_flat > 0.01]\n",
    "\n",
    "# Calculate the percentage\n",
    "percentage = (len(filtered_difference) / total_items) * 100\n",
    "\n",
    "# Print the percentage\n",
    "print(f\"Percentage of elements above 0.01: {percentage}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(indices1))\n",
    "print(type(reference_index_cpu))\n",
    "\n",
    "# Assuming you have a cuDF DataFrame (cudf_df) and a PyTorch Tensor (torch_tensor)\n",
    "# Extract the data from the cuDF DataFrame\n",
    "indices1_data = indices1.to_pandas().values\n",
    "\n",
    "# Convert the cuDF data to a PyTorch Tensor\n",
    "torch_indices1_data = torch.tensor(indices1_data).to(torch.int32)\n",
    "\n",
    "# Compare the content element-wise\n",
    "are_equal = torch.equal(torch_indices1_data, reference_index_cpu)\n",
    "\n",
    "if are_equal:\n",
    "    print(\"The content of the cuDF DataFrame and PyTorch Tensor is equal.\")\n",
    "else:\n",
    "    print(\"The content differs between the cuDF DataFrame and PyTorch Tensor.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare element-wise equality\n",
    "elementwise_equal = torch.eq(torch_indices1_data, reference_index_cpu)\n",
    "\n",
    "# Find indices where elements are different (i.e., where elementwise_equal is False)\n",
    "differing_indices = torch.nonzero(~elementwise_equal)\n",
    "\n",
    "\n",
    "print(differing_indices)\n",
    "print(torch_indices1_data)\n",
    "print(reference_index_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pointops2 import pointops2 as newerPointOps2\n",
    "\n",
    "def knn_query_pointopsv2(n_neighbors_input, point_cloud_input, n_points_input):\n",
    "    n_points_tensor = torch.cuda.FloatTensor([n_points_input])\n",
    "\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    reference_index, distance_indices = newerPointOps2.knnquery(n_neighbors_input, point_cloud_input, None, n_points_tensor, n_points_tensor)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    reference_index_cpu2 = reference_index.cpu()\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"C++ PointopsV2 KNN Execution time: {execution_time} seconds\")\n",
    "    # print(reference_index)\n",
    "\n",
    "    return reference_index_cpu2, execution_time, distance_indices\n",
    "\n",
    "reference_index_cpu2, execution_time_pointopsv2, distanceindex_pointopsv2 = knn_query_pointopsv2(n_neighbors, point_cloud, n_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(indices1))\n",
    "print(type(reference_index_cpu2))\n",
    "\n",
    "# Assuming you have a cuDF DataFrame (cudf_df) and a PyTorch Tensor (torch_tensor)\n",
    "# Extract the data from the cuDF DataFrame\n",
    "indices1_data = indices1.to_pandas().values\n",
    "\n",
    "# Convert the cuDF data to a PyTorch Tensor\n",
    "torch_indices1_data = torch.tensor(indices1_data).to(torch.int32)\n",
    "\n",
    "# Compare the content element-wise\n",
    "are_equal = torch.equal(torch_indices1_data, reference_index_cpu2)\n",
    "\n",
    "if are_equal:\n",
    "    print(\"The content of the cuDF DataFrame and PyTorch Tensor is equal.\")\n",
    "else:\n",
    "    print(\"The content differs between the cuDF DataFrame and PyTorch Tensor.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare element-wise equality\n",
    "elementwise_equal = torch.eq(torch_indices1_data, reference_index_cpu2)\n",
    "\n",
    "# Find indices where elements are different (i.e., where elementwise_equal is False)\n",
    "differing_indices2 = torch.nonzero(~elementwise_equal)\n",
    "\n",
    "\n",
    "print(differing_indices2)\n",
    "print(torch_indices1_data)\n",
    "print(reference_index_cpu2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.equal(differing_indices2, differing_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a cuML NearestNeighbors model\n",
    "# nn4 = NearestNeighbors(n_neighbors=n_neighbors, algorithm='ivfpq')\n",
    "# # Compute the Dask DataFrame\n",
    "# ddf_computed = ddf.compute()\n",
    "\n",
    "# start_time = timeit.default_timer()\n",
    "\n",
    "\n",
    "\n",
    "# # Fit the model with the input data\n",
    "# nn4.fit(ddf_computed)\n",
    "\n",
    "# # Get the nearest neighbors\n",
    "# distances, indices = nn4.kneighbors(ddf_computed)\n",
    "\n",
    "# end_time = timeit.default_timer()\n",
    "# execution_time = end_time - start_time\n",
    "# print(f\"ivfflat Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Execution times of each function\n",
    "execution_times = [execution_time_auto, execution_time_rbc, execution_time_ivfflat, execution_time_pointopsv1, execution_time_pointopsv2]\n",
    "\n",
    "# Function names\n",
    "function_names = ['autoSearchAlgorithm', 'rbcSearchAlgorithm', 'ivfflatSearchAlgorithm', 'knn_query_pointopsv1', 'knn_query_pointopsv2']\n",
    "\n",
    "# Number of n_points\n",
    "n_points_used = n_points\n",
    "\n",
    "# Plotting the execution times\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(function_names, execution_times)\n",
    "plt.xlabel('Function')\n",
    "plt.ylabel('Execution Time (seconds)')\n",
    "plt.title(f'Execution Time of Functions (n_points = {n_points_used})')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# List of n_points values\n",
    "n_points_list = [500000, 1000000, 1500000, 2000000, 2500000, 3000000, 3500000, 4000000, 4500000, 5000000, 5500000] # Do not use a number above 5500000 if your GPU Memory is 16GB\n",
    "\n",
    "# Lists to store execution times for each function\n",
    "execution_times_auto = []\n",
    "execution_times_rbc = []\n",
    "execution_times_ivfflat = []\n",
    "execution_times_pointopsv1 = []\n",
    "execution_times_pointopsv2 = []\n",
    "\n",
    "# Loop over n_points values\n",
    "for n_points in n_points_list:\n",
    "\n",
    "    print(f\"Current n_points: {n_points}\")\n",
    "\n",
    "    # Generate random data for the DataFrame\n",
    "    point_cloud = torch.cuda.FloatTensor(n_points, 3).uniform_()\n",
    "\n",
    "    # Generate random data for the DataFrame\n",
    "    random_data = np.array(point_cloud.cpu())\n",
    "\n",
    "    # Create the cudf.DataFrame\n",
    "    df = cudf.DataFrame(random_data, columns=[\"X\", \"Y\", \"Z\"])\n",
    "\n",
    "    # Call the functions and measure execution time\n",
    "    distances_auto, indices_auto, execution_time_auto = autoSearchAlgorithm(df)\n",
    "    distances_rbc, indices_rbc, execution_time_rbc = rbcSearchAlgorithm(df)\n",
    "    distances_ivfflat, indices_ivfflat, execution_time_ivfflat = ivfflatSearchAlgorithm(df, best_nlist, best_nprobe)\n",
    "    reference_index_cpu, execution_time_pointopsv1, distanceindex_pointopsv1 = knn_query_pointopsv1(n_neighbors, point_cloud, n_points)\n",
    "    reference_index_cpu2, execution_time_pointopsv2, distanceindex_pointopsv2 = knn_query_pointopsv2(n_neighbors, point_cloud, n_points)\n",
    "\n",
    "    # Compare element-wise equality\n",
    "    equal_auto_rbc = torch.eq(torch.tensor(indices_auto.values), torch.tensor(indices_rbc.values))\n",
    "    equal_auto_ivfflat = torch.eq(torch.tensor(indices_auto.values), torch.tensor(indices_ivfflat.values))\n",
    "    equal_rbc_ivfflat = torch.eq(torch.tensor(indices_rbc.values), torch.tensor(indices_ivfflat.values))\n",
    "\n",
    "    # Find indices where elements are different\n",
    "    differences_auto_rbc = torch.nonzero(~equal_auto_rbc)\n",
    "    differences_auto_ivfflat = torch.nonzero(~equal_auto_ivfflat)\n",
    "    differences_rbc_ivfflat = torch.nonzero(~equal_rbc_ivfflat)\n",
    "\n",
    "    # Print the number of differences\n",
    "    print(f\"Number of differences between indices_auto and indices_rbc: {len(differences_auto_rbc)}\")\n",
    "    print(f\"Number of differences between indices_auto and indices_ivfflat: {len(differences_auto_ivfflat)}\")\n",
    "    print(f\"Number of differences between indices_rbc and indices_ivfflat: {len(differences_rbc_ivfflat)}\")\n",
    "\n",
    "    # Append execution times to the respective lists\n",
    "    execution_times_auto.append(execution_time_auto)\n",
    "    execution_times_rbc.append(execution_time_rbc)\n",
    "    execution_times_ivfflat.append(execution_time_ivfflat)\n",
    "    execution_times_pointopsv1.append(execution_time_pointopsv1)\n",
    "    execution_times_pointopsv2.append(execution_time_pointopsv2)\n",
    "\n",
    "\n",
    "    # Compare element-wise equality\n",
    "    equal_distances_auto_rbc = torch.eq(torch.tensor(distances_auto.values), torch.tensor(distances_rbc.values))\n",
    "    equal_distances_auto_ivfflat = torch.eq(torch.tensor(distances_auto.values), torch.tensor(distances_ivfflat.values))\n",
    "    equal_distances_rbc_ivfflat = torch.eq(torch.tensor(distances_rbc.values), torch.tensor(distances_ivfflat.values))\n",
    "\n",
    "    # Find indices where elements are different\n",
    "    differences_distances_auto_rbc = torch.nonzero(~equal_distances_auto_rbc)\n",
    "    differences_distances_auto_ivfflat = torch.nonzero(~equal_distances_auto_ivfflat)\n",
    "    differences_distances_rbc_ivfflat = torch.nonzero(~equal_distances_rbc_ivfflat)\n",
    "\n",
    "    # Calculate the average distance error for the differences\n",
    "    average_distance_error_auto_rbc = torch.mean(torch.abs(torch.tensor(distances_auto.values)[differences_distances_auto_rbc] - torch.tensor(distances_rbc.values)[differences_distances_auto_rbc]))\n",
    "\n",
    "    average_distance_error_auto_ivfflat = torch.mean(torch.abs(torch.tensor(distances_auto.values)[differences_distances_auto_ivfflat] - torch.tensor(distances_ivfflat.values)[differences_distances_auto_ivfflat]))\n",
    "    \n",
    "    average_distance_error_rbc_ivfflat = torch.mean(torch.abs(torch.tensor(distances_rbc.values)[differences_distances_rbc_ivfflat] - torch.tensor(distances_ivfflat.values)[differences_distances_rbc_ivfflat]))\n",
    "\n",
    "    # Print the average distance errors\n",
    "    print(f\"Average distance error between distances_auto and distances_rbc: {average_distance_error_auto_rbc}\")\n",
    "    print(f\"Average distance error between distances_auto and distances_ivfflat: {average_distance_error_auto_ivfflat}\")\n",
    "    print(f\"Average distance error between distances_rbc and distances_ivfflat: {average_distance_error_rbc_ivfflat}\")\n",
    "\n",
    "# Create a DataFrame to store the execution times\n",
    "execution_times_df = pd.DataFrame({\n",
    "    'n_points': n_points_list,\n",
    "    'autoSearchAlgorithm': execution_times_auto,\n",
    "    'rbcSearchAlgorithm': execution_times_rbc,\n",
    "    'ivfflatSearchAlgorithm': execution_times_ivfflat,\n",
    "    'knn_query_pointopsv1': execution_times_pointopsv1,\n",
    "    'knn_query_pointopsv2': execution_times_pointopsv2\n",
    "})\n",
    "\n",
    "# Plotting the execution times\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(execution_times_df['n_points'], execution_times_df['autoSearchAlgorithm'], label='autoSearchAlgorithm')\n",
    "plt.plot(execution_times_df['n_points'], execution_times_df['rbcSearchAlgorithm'], label='rbcSearchAlgorithm')\n",
    "plt.plot(execution_times_df['n_points'], execution_times_df['ivfflatSearchAlgorithm'], label='ivfflatSearchAlgorithm')\n",
    "plt.plot(execution_times_df['n_points'], execution_times_df['knn_query_pointopsv1'], label='knn_query_pointopsv1')\n",
    "plt.plot(execution_times_df['n_points'], execution_times_df['knn_query_pointopsv2'], label='knn_query_pointopsv2')\n",
    "plt.xlabel('n_points')\n",
    "plt.ylabel('Execution Time (seconds)')\n",
    "plt.title('Execution Time Comparison for Different n_points')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the differences_auto_ivfflat to a pandas DataFrame\n",
    "df_differences = pd.DataFrame(torch.tensor(indices_ivfflat.values).numpy(), columns=['Index1', 'Index2'])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_differences.to_csv('differences_auto_ivfflat.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_times_df.to_csv('execution_times.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the CSV file\n",
    "execution_times_df = pd.read_csv('execution_times.csv')\n",
    "\n",
    "# Plotting the execution times\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(execution_times_df['n_points'], execution_times_df['autoSearchAlgorithm'], label='autoSearchAlgorithm')\n",
    "plt.plot(execution_times_df['n_points'], execution_times_df['rbcSearchAlgorithm'], label='rbcSearchAlgorithm')\n",
    "plt.plot(execution_times_df['n_points'], execution_times_df['ivfflatSearchAlgorithm'], label='ivfflatSearchAlgorithm')\n",
    "plt.plot(execution_times_df['n_points'], execution_times_df['knn_query_pointopsv1'], label='knn_query_pointopsv1')\n",
    "plt.plot(execution_times_df['n_points'], execution_times_df['knn_query_pointopsv2'], label='knn_query_pointopsv2')\n",
    "plt.xlabel('n_points')\n",
    "plt.ylabel('Execution Time (seconds)')\n",
    "plt.title('Execution Time Comparison for Different n_points')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Empty GPU memory usage of cuDF\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pointcept_py_3_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
