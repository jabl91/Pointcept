{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributed import Client\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import wait\n",
    "from cuml.dask.common.utils import persist_across_workers\n",
    "import dask_cudf\n",
    "\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "from cudf import DataFrame\n",
    "from cuml.dask.neighbors import NearestNeighbors\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask-CUDA configuration\n",
    "# os.environ[\"DASK_RMM__POOL_SIZE\"] = \"500M\"\n",
    "os.environ[\"DASK_UCX__CUDA_COPY\"] = \"True\"\n",
    "# os.environ[\"DASK_UCX__TCP\"] = \"True\"\n",
    "# os.environ[\"DASK_UCX__NVLINK\"] = \"True\"\n",
    "# os.environ[\"DASK_UCX__INFINIBAND\"] = \"True\"\n",
    "os.environ[\"DASK_UCX__NET_DEVICES\"] = \"ib0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCUDACluster(n_workers=1, threads_per_worker=512, rmm_pool_size=\"500M\", enable_tcp_over_ucx=True, enable_nvlink=True, enable_infiniband=True)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_data(client, np_array, n_workers=None, partitions_per_worker=1):\n",
    "    # Get workers on cluster\n",
    "    workers = list(client.has_what().keys())\n",
    "    # Select only n_workers workers\n",
    "    if n_workers:\n",
    "        workers = workers[:n_workers]\n",
    "    # Compute number of partitions\n",
    "    n_partitions = partitions_per_worker * len(workers)\n",
    "    # From host to device\n",
    "    cp_array = cp.array(np_array)\n",
    "    # From cuPy array to cuDF Dataframe\n",
    "    cudf_df = DataFrame(cp_array)\n",
    "    # From cuDF Dataframe to distributed Dask Dataframe\n",
    "    dask_cudf_df = dask_cudf.from_cudf(cudf_df, npartitions=n_partitions)\n",
    "    dask_cudf_df, = persist_across_workers(client, [dask_cudf_df], workers=workers)\n",
    "    wait(dask_cudf_df)\n",
    "    return dask_cudf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define index and query\n",
    "n_points = 65536\n",
    "index = np.random.rand(n_points, 3).astype(np.float32)\n",
    "\n",
    "# Distribute index and query\n",
    "dist_index = distribute_data(client, index, n_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 16\n",
    "\n",
    "# Create cuML distributed KNN model\n",
    "model = NearestNeighbors(client=client,\n",
    "                         n_neighbors=n_neighbors)\n",
    "# Fit model with index\n",
    "model.fit(dist_index)\n",
    "# Run search with a query\n",
    "distances, indices = model.kneighbors(dist_index)\n",
    "# Collect results back to the calling machine\n",
    "distances, indices = client.compute([distances, indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pointcept_py_3_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
